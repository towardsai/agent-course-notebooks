{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisfb01/agent-course-notebooks/blob/main/notebooks/lesson_16_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEpmNtjcwZ6y"
      },
      "source": [
        "# Lesson 16: FastMCP — MCP Server and Client Quickstart\n",
        "\n",
        "In this lesson, you will run a Model Context Protocol (MCP) server and MCP client using the FastMCP library, then explore how our research agent exposes MCP tools, MCP resources, and MCP prompts. We’ll start with a quick demo that runs the MCP client with an in-memory MCP server directly from this notebook, so you can get to try its capabilities immediately. Then, we’ll examine the MCP server and MCP client code structure.\n",
        "\n",
        "Learning Objectives:\n",
        "- Learn how to create an MCP server using `fastmcp`\n",
        "- Learn how to create an MCP client using `fastmcp`\n",
        "- Learn how to use the `fastmcp` library to expose MCP tools, MCP resources, and MCP prompts\n",
        "- Learn how to use the `fastmcp` library to interact with an MCP server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdORMi_swZ6y"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Run the following command to install all the required packages to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ansg3_z5wZ6z"
      },
      "outputs": [],
      "source": [
        "%pip install agentic-ai-engineering-course -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM84JU1-wZ6z"
      },
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the `Course Admin` lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env`\n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xEHgoGxwZ6z"
      },
      "outputs": [],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SxpnG_zwZ60"
      },
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5-a5UwgwZ60"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFYShnFjwZ60"
      },
      "source": [
        "## 2. Try the agent (MCP client quickstart)\n",
        "\n",
        "The research agent is made of an MCP server and an MCP client.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools, MCP resources, and MCP prompt via router modules. The MCP client is a `fastmcp` client that connects to the MCP server and allows you to interact with it, along with interacting with the LLM agent.\n",
        "\n",
        "This quickstart runs the MCP client of the research agent inside the notebook kernel. It connects to the MCP server running in‑memory (same process), which is the only transport supported for running everything in the same notebook. So, we'll always run the MCP server in-memory in the notebooks.\n",
        "\n",
        "Run the next code cell to start the MCP client. You will see some texts and can type commands directly in the input box that appears. The input box will be in different locations depending on where you are running the notebook from.\n",
        "\n",
        "Once the client is running, you can type commands when prompted, such as:\n",
        "\n",
        "- `/tools`: list all available MCP tools with names and descriptions.\n",
        "- `/resources`: list all available MCP resources with their URIs.\n",
        "- `/prompts`: list all available MCP prompts by name and description.\n",
        "- `/prompt/full_research_instructions_prompt`: fetch the research workflow prompt and inject it into the conversation.\n",
        "- `/resource/system://memory`: read and print the server memory stats (an example of running an MCP resource).\n",
        "- `/model-thinking-switch`: toggle model “thinking” traces on/off. By default it is true, which means that you'll see the agent's thoughts in the conversation before each answer or tool call.\n",
        "- Any other text: treated as a normal user message for the agent, which may use the MCP server tools for answering.\n",
        "- `/quit`: terminate the client.\n",
        "\n",
        "At first, try with the following commands and see what happens:\n",
        "- `Hello! Who are you?`\n",
        "- `/tools`\n",
        "- `/resource/system://memory`\n",
        "- `/quit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6gJm698wZ60"
      },
      "outputs": [],
      "source": [
        "# Run the MCP client in-kernel\n",
        "import sys\n",
        "\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "\n",
        "# Start client with in-memory server\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtKiwpn6wZ60"
      },
      "source": [
        "Whenever you want, you can run the previous cell again to try the client.\n",
        "\n",
        "Now, let's see how the MCP server works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SBPVHk6wZ61"
      },
      "source": [
        "## 3. MCP Server Overview\n",
        "\n",
        "The purpose of this section is to show how the MCP server is created with `fastmcp` and how it wires MCP tools, MCP resources, and MCP prompts.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools (actions with side effects like scraping webpages, transcribing videos, etc.), MCP resources (read-only endpoints for information like system status or memory), and MCP prompts (reusable instruction blocks, such as our agent workflow) via router modules.\n",
        "\n",
        "The MCP server follows a FastAPI‑like layout for clarity and scalability. It is structured as follows:\n",
        "\n",
        "- `server.py`: Entry point exposing `create_mcp_server()` and a `__main__` runner.\n",
        "- `routers/`: Functions that attach endpoints to the FastMCP instance.\n",
        "  - `tools.py`: registers all MCP tools.\n",
        "  - `resources.py`: registers all MCP resources.\n",
        "  - `prompts.py`: registers all MCP prompts.\n",
        "- `tools/`: MCP tools implementations.\n",
        "- `resources/`: MCP resources implementations.\n",
        "- `prompts/`: MCP prompts implementations (e.g. full workflow instructions for the agent).\n",
        "- `app/`: Functions implementing business logic.\n",
        "- `utils/`: Utility functions.\n",
        "- `config/`: Pydantic settings (`settings.py`) for server name/version, logging, model choices, and API keys.\n",
        "\n",
        "This separation keeps orchestration thin at the server boundary while allowing each capability (tool/resource/prompt) to evolve independently.\n",
        "\n",
        "Let's see now how the MCP server is created.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/server.py_\n",
        "\n",
        "```python\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "from .config.settings import settings\n",
        "from .routers.prompts import register_mcp_prompts\n",
        "from .routers.resources import register_mcp_resources\n",
        "from .routers.tools import register_mcp_tools\n",
        "\n",
        "\n",
        "def create_mcp_server() -> FastMCP:\n",
        "    \"\"\"\n",
        "    Create and configure the MCP server instance.\n",
        "\n",
        "    This function can be imported to get a configured MCP server\n",
        "    for use with in-memory transport in clients.\n",
        "\n",
        "    Returns:\n",
        "        FastMCP: Configured MCP server instance\n",
        "    \"\"\"\n",
        "    # Create the FastMCP server instance\n",
        "    mcp = FastMCP(\n",
        "        name=settings.server_name,\n",
        "        version=settings.version,\n",
        "    )\n",
        "\n",
        "    # Register all MCP endpoints\n",
        "    register_mcp_tools(mcp)\n",
        "    register_mcp_resources(mcp)\n",
        "    register_mcp_prompts(mcp)\n",
        "\n",
        "    return mcp\n",
        "```\n",
        "\n",
        "Notice how the `FastMCP` instance is created and how the `mcp` object is passed to the `register_mcp_tools`, `register_mcp_resources`, and `register_mcp_prompts` functions. It is pretty similar to how you would create a FastAPI app and attach endpoints to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKdJdDW8wZ61"
      },
      "source": [
        "### 3.1 Registering MCP Tools\n",
        "\n",
        "Let's see now in particular how to register an MCP tool with `fastmcp`. This specific tool reads the article guidelines and extracts relevant references. Its implementation is in the `tools/extract_guidelines_urls_tool.py` file, along with other business logic functions in the `app/` folder. You can read the full file `mcp_server/src/routers/tools.py` to see all the 11 available MCP tools.\n",
        "\n",
        "Source: _mcp_server/src/routers/tools.py_\n",
        "\n",
        "```python\n",
        "@mcp.tool()\n",
        "async def extract_guidelines_urls(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract URLs and local file references from article guidelines.\n",
        "\n",
        "    Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
        "    - GitHub URLs\n",
        "    - Other HTTP/HTTPS URLs\n",
        "    - Local file references (files mentioned in quotes with extensions)\n",
        "\n",
        "    Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
        "    \"\"\"\n",
        "    result = extract_guidelines_urls_tool(research_directory)\n",
        "    return result\n",
        "```\n",
        "\n",
        "This tool is the first step in the workflow. It reads the article guideline and writes a structured file containing URLs and local references. Notice how it requires a `research_directory` input, which is the path to the research directory containing a `article_guideline.md` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cthy5PM3wZ61"
      },
      "source": [
        "Let's test it with a sample article guideline. In the research agent folder, there's a `data/sample_research_folder` folder with an `article_guideline.md` file. Let's use it as input for the `extract_guidelines_urls` tool.\n",
        "\n",
        "\n",
        "> **Colab users:** if you installed the package in Google Colab, you can pass this absolute path directly to the MCP client:\n",
        ">\n",
        "> `/usr/local/lib/python3.12/dist-packages/research_agent_part_2/data/sample_research_folder`\n",
        "\n",
        "\n",
        "\n",
        "Here is how it is structured:\n",
        "\n",
        "```md\n",
        "## Global Context of the Lesson\n",
        "\n",
        "...\n",
        "\n",
        "## Lesson Outline\n",
        "\n",
        "## Section 1: Introduction\n",
        "\n",
        "...\n",
        "\n",
        "## Section 2: Understanding why agents need tools\n",
        "\n",
        "...\n",
        "\n",
        "## Section N: Conclusion\n",
        "\n",
        "...\n",
        "\n",
        "## Article code\n",
        "\n",
        "Links to code that will be used to support the article. Always prioritize this code over every other piece of code found in the sources:\n",
        "\n",
        "- [Notebook 1](https://github.com/path/to/notebook.ipynb)\n",
        "\n",
        "## Sources\n",
        "\n",
        "- [Function calling with the Gemini API](https://ai.google.dev/gemini-api/docs/function-calling)\n",
        "- [Function calling with OpenAI's API](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [Tool Calling Agent From Scratch](https://www.youtube.com/watch?v=ApoDzZP8_ck)\n",
        "- [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/pdf/2401.17464v3)\n",
        "- [Building AI Agents from scratch - Part 1: Tool use](https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part)\n",
        "- [What is Tool Calling? Connecting LLMs to Your Data](https://www.youtube.com/watch?v=h8gMhXYAv1k)\n",
        "- [ReAct vs Plan-and-Execute: A Practical Comparison of LLM Agent Patterns](https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9)\n",
        "- [Agentic Design Patterns Part 3, Tool Use](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/)\n",
        "```\n",
        "\n",
        "Normally, an `article_guideline.md` file would contain detailed information about the article to write, including the outline, the sections, the sources, and the code, as the research agent needs this information to look for the best content to include in the article. In this sample file, we have a simplified version of an article guideline.\n",
        "\n",
        "Now, run the next code cell to run the research agent MCP client again, and give it the following command. Make sure to replace the folder path with your actual absolute folder path, otherwise the tool will not find the file.\n",
        "- Command to give to the client: `Run the \"extract_guidelines_urls\" tool with the \"data/sample_research_folder\" directory as research folder and stop after the tool has finished running.`.\n",
        "\n",
        "If you are on **Colab**, you can instead pass the absolute path:\n",
        "`/usr/local/lib/python3.12/dist-packages/research_agent_part_2/data/sample_research_folder`\n",
        "\n",
        "In case you provide the wrong path, notice how the tool will return an error and how the agent will ask you to provide a valid path and how to proceed.\n",
        "\n",
        "*Important*: the agent will manage every message starting with the `/` as a command, so, if you want to provide the folder path in a message, you need to write something like this: `Here is the folder path: /absolute/path/to/the/folder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z_qfkxLwZ61"
      },
      "outputs": [],
      "source": [
        "# Run the MCP client in-kernel\n",
        "import sys\n",
        "\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "\n",
        "# Start client with in-memory server\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOMc4K26wZ61"
      },
      "source": [
        "Notice the agent's thoughts. If everything ran correctly, you'll see the text \"Tool execution successful\". If so, notice that there is a new folder named `.nova` in the research directory, with a file `guidelines_filenames.json` inside. This file contains the URLs and local references extracted from the article guideline.\n",
        "\n",
        "Its content should be like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"github_urls\": [\n",
        "    \"https://github.com/path/to/notebook.ipynb\"\n",
        "  ],\n",
        "  \"youtube_videos_urls\": [\n",
        "    \"https://www.youtube.com/watch?v=ApoDzZP8_ck\",\n",
        "    \"https://www.youtube.com/watch?v=h8gMhXYAv1k\"\n",
        "  ],\n",
        "  \"other_urls\": [\n",
        "    \"https://ai.google.dev/gemini-api/docs/function-calling\",\n",
        "    \"https://platform.openai.com/docs/guides/function-calling\",\n",
        "    \"https://arxiv.org/pdf/2401.17464v3\",\n",
        "    \"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\n",
        "    \"https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/\"\n",
        "  ],\n",
        "  \"local_file_paths\": []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmaDEjNLwZ61"
      },
      "source": [
        "So, the tool has extracted those URLs from the `article_guideline.md` file and categorized them into the groups you see above.\n",
        "\n",
        "We can run the above tool also programmatically as follows. The output shows the result of running it from the local setup of the author of this notebook. To run it, update the path of the `research_folder` variable with your absolute path to the `sample_research_folder` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPJs0MwGwZ61"
      },
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import extract_guidelines_urls_tool\n",
        "\n",
        "# On Colab you can use this path \"/usr/local/lib/python3.12/dist-packages/research_agent_part_2/data/sample_research_folder\n",
        "research_folder = \"/your/absolute/path/to/sample_research_folder\"\n",
        "extract_guidelines_urls_tool(research_folder=research_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7AKar_RwZ61"
      },
      "source": [
        "We'll comment the output of this tool in the next lesson. In the next lessons, we'll run each tool one by one like in the above code cell, so you can see the output of each tool and understand how the research agent works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMJmZg63wZ61"
      },
      "source": [
        "### 3.2 Registering MCP Resources\n",
        "\n",
        "Let's see now how to register an MCP resource endpoint using `fastmcp`.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_server/src/routers/resources.py_\n",
        "\n",
        "```python\n",
        "@mcp.resource(\"system://memory\")\n",
        "async def memory_usage() -> Dict[str, Any]:\n",
        "    \"\"\"Monitor memory usage of the server.\"\"\"\n",
        "    return await get_memory_usage_resource()\n",
        "```\n",
        "\n",
        "It's very similar to how tools are registered, except that the `@mcp.resource()` decorator is used instead of the `@mcp.tool()` decorator.\n",
        "\n",
        "Let's now run the `get_memory_usage_resource` function to see the memory usage of the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWkg_heJwZ61"
      },
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.resources import get_memory_usage_resource\n",
        "\n",
        "await get_memory_usage_resource()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u868_xc8wZ61"
      },
      "source": [
        "This output is the same output that an MCP client would get if it uses this MCP resource.\n",
        "\n",
        "*Important*: in the research agent MCP client, we have only implemented the use of tools by the agent LLM, but we could have implemented the use of resources as well. Most MCP clients do not support resources yet, but their support is increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pv5XK6JwZ61"
      },
      "source": [
        "### 3.3 Registering MCP Prompts\n",
        "\n",
        "This section shows how MCP prompts are implemented with `fastmcp`. This specific prompt defines the agentic workflow for the research agent.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/routers/prompts.py_\n",
        "\n",
        "```python\n",
        "@mcp.prompt()\n",
        "async def full_research_instructions_prompt() -> str:\n",
        "    \"\"\"Complete Nova research agent workflow instructions.\"\"\"\n",
        "    return await _get_research_instructions()\n",
        "```\n",
        "\n",
        "The prompt content encodes the full workflow orchestration the agent should follow when started via a prompt.\n",
        "\n",
        "In practice, MCP prompts are triggered by users from an MCP client, not by the agent LLM. When a user triggers an MCP prompt, the MCP client would retrieve that prompt and load it to instruct the LLM on how to run the available tools in sequence (and sometimes in parallel) according to the workflow described in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XBGnUAAwZ62"
      },
      "source": [
        "For reference, here is the full prompt content of the only MCP prompt implemented in the research agent, which is the `full_research_instructions_prompt` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSvtb-AQwZ62"
      },
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.prompts import (\n",
        "    full_research_instructions_prompt,\n",
        ")\n",
        "\n",
        "prompt = await full_research_instructions_prompt()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMtCUOamwZ62"
      },
      "source": [
        "This is the instruction block that defines the agentic workflow for the research agent. In the next lessons, we'll go through each step defined in the workflow, learn how it is implemented, and run it in isolation.\n",
        "\n",
        "Let's now see how the MCP client works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T0wsAB4wZ62"
      },
      "source": [
        "## 4. MCP Client Overview\n",
        "\n",
        "Here is the MCP client's layout. It is structured as follows:\n",
        "\n",
        "- `client.py`: CLI entry point. Parses `--transport`, creates the client (in‑memory or stdio), fetches capabilities, prints the startup banner, and runs the interactive loop.\n",
        "- `settings.py`: Centralized Pydantic settings for API keys, model selection, logging, transport, and server paths.\n",
        "- `utils/`: Helper modules used by `client.py`.\n",
        "\n",
        "The MCP client can run with two transports:\n",
        "\n",
        "- **in-memory**: The client imports the server factory (the `create_mcp_server` function from the `client.py` file) and instantiates the server inside the same Python process. This is fast, simple to debug, and is what we use in this notebook.\n",
        "- **stdio**: The client launches the server as a separate process and communicates using the MCP stdio transport. This mirrors how external MCP clients (e.g., editors) connect to servers and provides process isolation.\n",
        "\n",
        "Let's see how the code of the `client.py` file works.\n",
        "\n",
        "Source: _mcp_client/src/client.py_\n",
        "\n",
        "```python\n",
        "if args.transport == \"in-memory\":\n",
        "    ...\n",
        "    from mcp_server.src.server import create_mcp_server\n",
        "    mcp_server = create_mcp_server()\n",
        "    mcp_client = Client(mcp_server)\n",
        "\n",
        "elif args.transport == \"stdio\":\n",
        "    config = {\n",
        "        \"mcpServers\": {\n",
        "            \"research-agent\": {\n",
        "                \"transport\": \"stdio\",\n",
        "                \"command\": \"uv\",\n",
        "                \"args\": [\n",
        "                    \"--directory\", str(settings.server_main_path),\n",
        "                    \"run\", \"-m\", \"src.server\",\n",
        "                    \"--transport\", \"stdio\",\n",
        "                ],\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    mcp_client = Client(config)\n",
        "\n",
        "# At startup\n",
        "tools, resources, prompts = await get_capabilities_from_mcp_client(mcp_client)\n",
        "print_startup_info(tools, resources, prompts)\n",
        "\n",
        "async with mcp_client:\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"👤 You: \").strip()\n",
        "        ...\n",
        "\n",
        "        # Parse input\n",
        "        parsed_input = parse_user_input(user_input)\n",
        "        ...\n",
        "\n",
        "        # Dispatch handling\n",
        "        await handle_user_message(parsed_input=parsed_input, ...)\n",
        "        ...\n",
        "```\n",
        "\n",
        "It does the following:\n",
        "1) Parse the `--transport` flag.\n",
        "2) If in-memory, build a `Client` with the FastMCP server object. If stdio, pass a config that tells FastMCP how to exec the server via `uv`.\n",
        "3) Query the MCP server for its capabilities (tools/resources/prompts) and print them.\n",
        "4) Enter the interactive loop: read input, parse it, and dispatch handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnbbymdwZ62"
      },
      "source": [
        "The code above is run when the MCP client is started. If you remember from previous cells, when the MCP client is started, it prints the following information:\n",
        "\n",
        "```\n",
        "🛠️ Available tools: 11\n",
        "📚 Available resources: 2\n",
        "💬 Available prompts: 1\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
        "```\n",
        "\n",
        "But, how does the MCP client know how many tools, resources, and prompts are available? Let's see how the `get_capabilities_from_mcp_client` function works.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/mcp_startup_utils.py_\n",
        "\n",
        "```python\n",
        "async def get_capabilities_from_mcp_client(client: Client) -> tuple[List, List, List]:\n",
        "    \"\"\"Get available capabilities.\"\"\"\n",
        "    async with client:\n",
        "        tools = await client.list_tools()\n",
        "        resources = await client.list_resources()\n",
        "        prompts = await client.list_prompts()\n",
        "\n",
        "    return tools, resources, prompts\n",
        "```\n",
        "\n",
        "As you can see, the MCP client object has a `list_tools`, `list_resources`, and `list_prompts` method that returns the list of tools, resources, and prompts respectively. These lists contain information about their names, descriptions, parameters, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJMJUwtKwZ62"
      },
      "source": [
        "We are now ready to learn how the MCP client parses the user input and how it handles the user messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmM3knuDwZ62"
      },
      "source": [
        "### 4.1 Parsing Input and Commands\n",
        "\n",
        "The client supports a small command language. Input can be either a command (starting with `/`) or a freeform user message.\n",
        "\n",
        "Possible commands are:\n",
        "- `/tools`, `/resources`, `/prompts`\n",
        "- `/prompt/<name>` (e.g., `/prompt/full_research_instructions_prompt`)\n",
        "- `/resource/<uri>` (e.g., `/resource/system://status`)\n",
        "- `/model-thinking-switch`\n",
        "- `/quit`\n",
        "\n",
        "The `parse_user_input` function simply classifies the input (no side effects) and it returns a `ProcessedInput` with metadata. Here are some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PLeI-fywZ62"
      },
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_client.src.utils.parse_message_utils import (\n",
        "    parse_user_input,\n",
        ")\n",
        "\n",
        "processed_input = parse_user_input(\"/tools\")\n",
        "print(processed_input.input_type)\n",
        "\n",
        "processed_input = parse_user_input(\"/resources\")\n",
        "print(processed_input.input_type)\n",
        "\n",
        "processed_input = parse_user_input(\"/prompt/full_research_instructions_prompt\")\n",
        "print(processed_input.input_type, processed_input.prompt_name)\n",
        "\n",
        "processed_input = parse_user_input(\"Hello, how are you?\")\n",
        "print(processed_input.input_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMX4q1IZwZ62"
      },
      "source": [
        "These processed inputs are then used to dispatch the correct handling.\n",
        "\n",
        "The `handle_user_message` function orchestrates the conversation, calling the appropriate helper for the parsed command, or appending a normal message and running the agent loop.\n",
        "\n",
        "Here are some examples. Let's first create the MCP server and client, and get the server capabilities (available tools, resources, and prompts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_h-QDlGwZ62"
      },
      "outputs": [],
      "source": [
        "from fastmcp import Client\n",
        "from research_agent_part_2.mcp_client.src.utils.handle_message_utils import (\n",
        "    handle_user_message,\n",
        ")\n",
        "from research_agent_part_2.mcp_client.src.utils.mcp_startup_utils import (\n",
        "    get_capabilities_from_mcp_client,\n",
        ")\n",
        "from research_agent_part_2.mcp_server.src.server import create_mcp_server\n",
        "\n",
        "# Create the MCP server and client\n",
        "mcp_server = create_mcp_server()\n",
        "mcp_client = Client(mcp_server)\n",
        "\n",
        "# Get the MCP server capabilities\n",
        "tools, resources, prompts = await get_capabilities_from_mcp_client(mcp_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45CBUdURwZ62"
      },
      "source": [
        "Now, let's parse the user input and handle the user message with the `handle_user_message` function. Here is an example with commands (i.e. messages starting with `/`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Pbs5EQwZ62"
      },
      "outputs": [],
      "source": [
        "# Parse the user input\n",
        "processed_input = parse_user_input(\"/resources\")\n",
        "conversation_history = []\n",
        "response = await handle_user_message(\n",
        "    processed_input,\n",
        "    tools,\n",
        "    resources,\n",
        "    prompts,\n",
        "    conversation_history,\n",
        "    mcp_client,\n",
        "    thinking_enabled=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbsqfc5RwZ62"
      },
      "source": [
        "The `handle_user_message` function is basically a router that calls the appropriate helper for the parsed message. It is defined in the `handle_message_utils.py` file, you can read it to learn more about it.\n",
        "\n",
        "As previously explained, the `tools` object contains the list of tools registered in the MCP server, retrieved by the `list_tools` method. If the input is of type `COMMAND_INFO_TOOLS`, the `handle_command` function is called.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/command_utils.py_\n",
        "\n",
        "```python\n",
        "def handle_command(processed_input: ProcessedInput, tools: List, resources: List, prompts: List):\n",
        "    \"\"\"Handle informational commands.\n",
        "\n",
        "    This function only handles informational commands (COMMAND_INFO_* types).\n",
        "    \"\"\"\n",
        "    if processed_input.input_type == InputType.COMMAND_INFO_TOOLS:\n",
        "        print_header(\"🛠️  Available Tools\")\n",
        "        for i, tool in enumerate(tools, 1):\n",
        "            print_item(tool.name, tool.description, i, Color.BRIGHT_WHITE, Color.YELLOW)\n",
        "    ...\n",
        "```\n",
        "\n",
        "This function retrieves, from each tool, the name and description, and prints them in a pretty format.\n",
        "\n",
        "All the tools are managed in a similar way.\n",
        "\n",
        "If the input message is of type `NORMAL_MESSAGE`, the `handle_agent_loop` function is called instead, which manages the agent loop for tool execution. Let's see how it works.\n",
        "\n",
        "Source:\n",
        "_mcp_client/src/utils/handle_agent_loop_utils.py_\n",
        "\n",
        "```python\n",
        "async def handle_agent_loop(\n",
        "    conversation_history: List[types.Content],\n",
        "    tools: List,\n",
        "    client: Client,\n",
        "    thinking_enabled: bool,\n",
        "):\n",
        "    \"\"\"Handle the agent loop for tool execution.\"\"\"\n",
        "    # Initialize LLM client\n",
        "    llm_config = build_llm_config_with_tools(tools, thinking_enabled)\n",
        "    llm_client = LLMClient(settings.model_id, llm_config)\n",
        "\n",
        "    while True:\n",
        "        print()\n",
        "        # Call LLM with current conversation history\n",
        "        response = await llm_client.generate_content(conversation_history)\n",
        "\n",
        "        # Extract and display thoughts as separate message (only if enabled)\n",
        "        if thinking_enabled:\n",
        "            thoughts = extract_thought_summary(response)\n",
        "            ...\n",
        "\n",
        "        # Check for function calls\n",
        "        function_call_info = extract_first_function_call(response)\n",
        "        if function_call_info:\n",
        "            name, args = function_call_info\n",
        "\n",
        "            # Check if this is a tool call\n",
        "            is_tool = any(tool.name == name for tool in tools)\n",
        "\n",
        "            if is_tool:\n",
        "                ...\n",
        "\n",
        "                # Execute the tool via MCP server\n",
        "                tool_result = await execute_tool(name, args, client)\n",
        "                # Add tool result to conversation history\n",
        "                tool_response = f\"Tool '{name}' executed successfully. Result: {tool_result}\"\n",
        "                conversation_history.append(types.Content(role=\"user\", parts=[types.Part(text=tool_response)]))\n",
        "                ...\n",
        "        else:\n",
        "            # Extract final text response - this ends the ReAct loop\n",
        "            final_text = extract_final_answer(response)\n",
        "            conversation_history.append(response.candidates[0].content)\n",
        "            ...\n",
        "            break  # Exit the agent loop\n",
        "```\n",
        "\n",
        "This function is the main loop that manages the agent loop for tool execution. It initializes the LLM client, builds the LLM configuration with the tools, and then enters the agent loop.\n",
        "\n",
        "The loop is structured as follows:\n",
        "\n",
        "1) Call the LLM with the current conversation history.\n",
        "2) Extract and display thoughts as separate message (only if enabled).\n",
        "3) Check for function calls.\n",
        "4) If there is a function call, check if it is a tool call.\n",
        "5) If it is a tool call, execute the tool via MCP server.\n",
        "6) Add the tool result to the conversation history.\n",
        "\n",
        "The `LLMClient` class is simply a wrapper class that allows to generate content (or a function call) with an LLM, independently from the specific LLM provider. Right now it only implements Google Gemini as model, but it can be easily extended to other models. It is defined in the `llm_utils.py` file.\n",
        "\n",
        "The `build_llm_config_with_tools` function builds the LLM configuration with the tools, it only works with Gemini for now. It is defined in the `llm_utils.py` file as well. Here's its code.\n",
        "\n",
        "```python\n",
        "def build_llm_config_with_tools(mcp_tools: List, thinking_enabled: bool = True) -> types.GenerateContentConfig:\n",
        "    \"\"\"Build Gemini config with all MCP tools converted to Gemini format.\"\"\"\n",
        "    gemini_tools = []\n",
        "\n",
        "    for tool in mcp_tools:\n",
        "        gemini_tool = types.Tool(\n",
        "            function_declarations=[\n",
        "                types.FunctionDeclaration(\n",
        "                    name=tool.name,\n",
        "                    description=tool.description,\n",
        "                    parameters=tool.inputSchema,\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        gemini_tools.append(gemini_tool)\n",
        "\n",
        "    # Create thinking config dynamically based on current state\n",
        "    thinking_config = types.ThinkingConfig(\n",
        "        include_thoughts=thinking_enabled,\n",
        "        thinking_budget=settings.thinking_budget,\n",
        "    )\n",
        "\n",
        "    return types.GenerateContentConfig(\n",
        "        tools=gemini_tools,\n",
        "        thinking_config=thinking_config,\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\n",
        "    )\n",
        "```\n",
        "\n",
        "The code above basically instructions the LLM to leverage thinking (if enabled) with the specificed thinking budget (i.e. the maximum number of tokens the LLM can use to think) and to use the available tools from the MCP server.\n",
        "\n",
        "The other functions from the `handle_agent_loop` function, like `extract_thought_summary` and `extract_final_answer`, are used to extract the thoughts and the final answer from the LLM response. It's boilerplate code that works for Gemini and can be copypasted for other projects.\n",
        "\n",
        "The `execute_tool` function is used to execute the tool via MCP server. It is defined in the `handle_agent_loop_utils.py` file. Here's its code.\n",
        "\n",
        "```python\n",
        "async def execute_tool(name: str, args: dict, client: Client):\n",
        "    \"\"\"Execute a tool and return the result.\"\"\"\n",
        "    ...\n",
        "    tool_result = await client.call_tool(name, args)\n",
        "    return tool_result\n",
        "```\n",
        "\n",
        "It uses the `call_tool` method of the `Client` object to execute the tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8IxVxQgwZ63"
      },
      "source": [
        "We can now test the MCP client with a user message that involves tool execution and see how the agent behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buMV_R80wZ63"
      },
      "outputs": [],
      "source": [
        "# Parse the user input\n",
        "path_to_research_folder = \"/your/absolute/path/to/sample_research_folder\"\n",
        "message = (\n",
        "    f\"Call the 'extract_guidelines_urls' tool with the '{path_to_research_folder}' directory as research folder, and stop after the tool has finished running.\"\n",
        "    \"Don't run any other tool after the 'extract_guidelines_urls' tool has finished running.\"\n",
        "    \"If the tool fails, explain to me the error message.\"\n",
        ")\n",
        "processed_input = parse_user_input(message)\n",
        "conversation_history = []\n",
        "async with mcp_client:\n",
        "    response = await handle_user_message(\n",
        "        processed_input,\n",
        "        tools,\n",
        "        resources,\n",
        "        prompts,\n",
        "        conversation_history,\n",
        "        mcp_client,\n",
        "        thinking_enabled=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoEqlyAbwZ63"
      },
      "source": [
        "We are good to go!\n",
        "\n",
        "In the next lesson, we'll learn more about how the MCP prompt is used by the MCP client to orchestrate the agentic workflow.\n",
        "Then, we'll go through each step of the research agent workflow, and we'll see how to run each tool in isolation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
